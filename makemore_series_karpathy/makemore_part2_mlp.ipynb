{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1c929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c7bd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in all the words.\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61041ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc080b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build a vocab of characters and mappings to/from integers.\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d9c6d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset.\n",
    "\n",
    "# We can update the block_size to get more context.\n",
    "block_size = 3            # Context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]    # Crop and append.\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39bfaad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "885390c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will embed the 27 chars in a 2-dimensional space.\n",
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15f88992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5317, -0.8100],\n",
       "        [ 0.0686, -0.4340],\n",
       "        [ 0.6754, -0.1459],\n",
       "        [ 0.4900, -2.1244],\n",
       "        [ 0.3606,  0.8209],\n",
       "        [-1.4012, -0.4754],\n",
       "        [ 0.1316,  0.4431],\n",
       "        [-0.2416, -0.1228],\n",
       "        [ 0.7791,  1.1401],\n",
       "        [-0.1821,  0.0800],\n",
       "        [ 1.2458,  0.1638],\n",
       "        [-0.1977,  0.1500],\n",
       "        [ 0.6284,  0.9660],\n",
       "        [ 0.3503, -1.3784],\n",
       "        [-0.4973,  0.2641],\n",
       "        [ 0.1854, -0.9773],\n",
       "        [ 0.0410,  0.7770],\n",
       "        [ 0.2522, -0.6684],\n",
       "        [ 0.7541, -0.3799],\n",
       "        [-1.3384,  1.1626],\n",
       "        [ 0.9145,  0.3126],\n",
       "        [ 0.4282,  1.1034],\n",
       "        [-0.2724,  0.1731],\n",
       "        [ 0.5598,  0.2846],\n",
       "        [-0.1600, -0.8552],\n",
       "        [ 0.4487,  0.2387],\n",
       "        [-0.4292, -2.3721]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7af3da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4012, -0.4754])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take the emb for the ix=5.\n",
    "# One way to get it is just pluck the row.\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3348700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4012, -0.4754])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to do it is by having a one-hot encoding then multiply it with the emb matrix\n",
    "# (think of it like a weight matrix).\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C\n",
    "\n",
    "# We will get the same thing as above because of matrix multiplication works.\n",
    "# But for this lecture we will just use the way it is done in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0adcdb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4012, -0.4754],\n",
       "        [ 0.1316,  0.4431],\n",
       "        [-0.2416, -0.1228]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In pytorch we can index lists. Infact it can be tensors.\n",
    "C[torch.tensor([5, 6, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfc3559c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It also works with 2D tensors. So we can directly plug in tensor X and get the embeddings for the chars.\n",
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0fcb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1357cf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0686, -0.4340])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02d30d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0686, -0.4340])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As can be seen, it plucks out the emb for 1.\n",
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35503830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5183426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights and biases of the neurons (MLP).\n",
    "# We are working with the figure 1 on page 6 of this paper:\n",
    "# https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "# \"A Neural Probabilistic Language Model\"\n",
    "# This is also available in the \"relevant_papers\" folder on the repo.\n",
    "\n",
    "# 6 input dim because 3*2=6 -- from the shape above. These are the input values to the neurons.\n",
    "# 100 are the output dims -- hyperparameter.\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2694aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function gives us a list of all the tensors along a dim.\n",
    "len(torch.unbind(emb, 1))     # --> We have a context_len of 3. This changes with block_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31399959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb, 1), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01c733c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is however a significantly better way to do this...\n",
    "# Let's see it with a toy example -- learning `view` in pytorch.\n",
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b27f56f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f8e5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can just help to reshape the tensor as we want.\n",
    "b = a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42e2bb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78799e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5317, -0.8100, -0.5317, -0.8100, -0.5317, -0.8100],\n",
       "        [-0.5317, -0.8100, -0.5317, -0.8100, -1.4012, -0.4754],\n",
       "        [-0.5317, -0.8100, -1.4012, -0.4754,  0.3503, -1.3784],\n",
       "        [-1.4012, -0.4754,  0.3503, -1.3784,  0.3503, -1.3784],\n",
       "        [ 0.3503, -1.3784,  0.3503, -1.3784,  0.0686, -0.4340],\n",
       "        [-0.5317, -0.8100, -0.5317, -0.8100, -0.5317, -0.8100],\n",
       "        [-0.5317, -0.8100, -0.5317, -0.8100,  0.1854, -0.9773],\n",
       "        [-0.5317, -0.8100,  0.1854, -0.9773,  0.6284,  0.9660],\n",
       "        [ 0.1854, -0.9773,  0.6284,  0.9660, -0.1821,  0.0800],\n",
       "        [ 0.6284,  0.9660, -0.1821,  0.0800, -0.2724,  0.1731],\n",
       "        [-0.1821,  0.0800, -0.2724,  0.1731, -0.1821,  0.0800],\n",
       "        [-0.2724,  0.1731, -0.1821,  0.0800,  0.0686, -0.4340],\n",
       "        [-0.5317, -0.8100, -0.5317, -0.8100, -0.5317, -0.8100],\n",
       "        [-0.5317, -0.8100, -0.5317, -0.8100,  0.0686, -0.4340],\n",
       "        [-0.5317, -0.8100,  0.0686, -0.4340, -0.2724,  0.1731],\n",
       "        [ 0.0686, -0.4340, -0.2724,  0.1731,  0.0686, -0.4340],\n",
       "        [-0.5317, -0.8100, -0.5317, -0.8100, -0.5317, -0.8100],\n",
       "        [-0.5317, -0.8100, -0.5317, -0.8100, -0.1821,  0.0800],\n",
       "        [-0.5317, -0.8100, -0.1821,  0.0800, -1.3384,  1.1626],\n",
       "        [-0.1821,  0.0800, -1.3384,  1.1626,  0.0686, -0.4340],\n",
       "        [-1.3384,  1.1626,  0.0686, -0.4340,  0.6754, -0.1459],\n",
       "        [ 0.0686, -0.4340,  0.6754, -0.1459, -1.4012, -0.4754],\n",
       "        [ 0.6754, -0.1459, -1.4012, -0.4754,  0.6284,  0.9660],\n",
       "        [-1.4012, -0.4754,  0.6284,  0.9660,  0.6284,  0.9660],\n",
       "        [ 0.6284,  0.9660,  0.6284,  0.9660,  0.0686, -0.4340],\n",
       "        [-0.5317, -0.8100, -0.5317, -0.8100, -0.5317, -0.8100],\n",
       "        [-0.5317, -0.8100, -0.5317, -0.8100, -1.3384,  1.1626],\n",
       "        [-0.5317, -0.8100, -1.3384,  1.1626,  0.1854, -0.9773],\n",
       "        [-1.3384,  1.1626,  0.1854, -0.9773,  0.0410,  0.7770],\n",
       "        [ 0.1854, -0.9773,  0.0410,  0.7770,  0.7791,  1.1401],\n",
       "        [ 0.0410,  0.7770,  0.7791,  1.1401, -0.1821,  0.0800],\n",
       "        [ 0.7791,  1.1401, -0.1821,  0.0800,  0.0686, -0.4340]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now applying the view on the emb.\n",
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5f27f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the computation for the hidden layer.\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)     # -1 makes pytorch infer that dimension given the other dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c9c221f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9999, -0.9938,  0.8547,  ..., -0.8765, -0.9007, -0.6529],\n",
       "        [ 0.9996, -0.9852,  0.8054,  ...,  0.3414, -0.4571, -0.9464],\n",
       "        [ 1.0000, -0.9967,  0.9458,  ..., -0.9964, -0.9989,  0.3555],\n",
       "        ...,\n",
       "        [-0.9621,  0.8756, -0.9909,  ..., -0.2411,  0.9890,  0.9849],\n",
       "        [ 0.3789,  0.9185,  0.2037,  ...,  0.9921,  0.9885,  0.9092],\n",
       "        [ 0.9514, -0.3889,  0.9030,  ...,  0.8777, -0.7336,  0.9897]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numbers are all between -1 and +1 due to the tanh.\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "395d791e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "981e0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the softmax layer. There are 27 output chars.\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bdc2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c25f2074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb1ceda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc16879d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6500125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row will sum 1.\n",
    "prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dae03802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.6672e-13, 2.5447e-07, 3.0782e-01, 1.6771e-12, 1.5385e-09, 8.3130e-07,\n",
       "        1.3691e-02, 1.3655e-11, 3.8660e-09, 2.6718e-08, 2.3409e-09, 9.7448e-12,\n",
       "        2.5197e-15, 2.7106e-08, 2.3674e-11, 8.1370e-12, 1.7140e-08, 3.7008e-13,\n",
       "        6.3204e-04, 1.4228e-09, 6.3568e-07, 1.0343e-03, 2.7549e-11, 1.0137e-03,\n",
       "        3.6177e-13, 7.5531e-11, 2.6803e-07, 1.9524e-11, 5.9927e-11, 1.7716e-08,\n",
       "        6.3148e-04, 1.5564e-12])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pluck out the probs assigned by the model to the correct label given by Y.\n",
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55f8f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NLL\n",
    "loss = -prob[torch.arange(32), Y].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad45b2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.1398)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d70a4c",
   "metadata": {},
   "source": [
    "## Cleaning up and re-writing. Also changing the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dbcd661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the entire dataset.\n",
    "block_size = 3            # Context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]    # Crop and append.\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81f8f084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# Restructuring the above code to be able to read data and create training, dev and test splits.\n",
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "\n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b3bc89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]), torch.Size([182625]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "56df0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 10), generator=g)\n",
    "W1 = torch.randn((30, 200), generator=g)\n",
    "b1 = torch.randn(200, generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f21f87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "55268a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9dcb5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b0fca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1285746097564697\n"
     ]
    }
   ],
   "source": [
    "for i in range(50000):\n",
    "  \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 10)\n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 200)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    # In the video we empirically determine that 0.1 is a good LR to use (around 50 mins in the video).\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    # lr = 0.01 -- Did this when we were running the loop manually multiple times.\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "        \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b1e344f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f79d8d7e3a0>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bf7f6304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2138, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The loss above would be for a mini-batch. But this one is for the entire dataset.\n",
    "emb = C[Xdev] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "469c664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mora.\n",
      "kayah.\n",
      "see.\n",
      "mel.\n",
      "ryalayethan.\n",
      "endrlee.\n",
      "aderedieliigh.\n",
      "poren.\n",
      "eden.\n",
      "van.\n",
      "aar.\n",
      "katelvontelin.\n",
      "shdonrgihimies.\n",
      "kindreelynn.\n",
      "nocarmon.\n",
      "zen.\n",
      "dariyah.\n",
      "faeha.\n",
      "kay.\n",
      "mus.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
